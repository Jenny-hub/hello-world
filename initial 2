# Install the pyLDAvis library
!pip install pyLDAvis
-----------------------
!python -m spacy download en_core_web_sm
---------------------------------------
#importing required libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import re
import nltk
import warnings
import string
import spacy
import gensim

import pyLDAvis.gensim
import pyLDAvis.gensim_models as gensimvis

from sklearn.model_selection import StratifiedKFold, train_test_split
from mlxtend.plotting import plot_confusion_matrix
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.metrics import accuracy_score,confusion_matrix,classification_report

from gensim import corpora, models
from gensim.models import Word2Vec
from gensim.similarities import MatrixSimilarity
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import LancasterStemmer, WordNetLemmatizer
from collections import Counter
from gensim.matutils import corpus2csc, sparse2full, corpus2dense
from wordcloud import WordCloud
from sklearn.utils import resample
from wordcloud import WordCloud,STOPWORDS
from gensim.models.coherencemodel import CoherenceModel

%matplotlib inline

nltk.download('stopwords')
stop_words = stopwords.words('english')
nlp = spacy.load('en_core_web_sm')

# Visualize the topics
pyLDAvis.enable_notebook()
warnings.simplefilter(action='ignore', category=FutureWarning)
warnings.filterwarnings("ignore", category=DeprecationWarning)
----------------------------------------------------------------------------------------
~~~~~~~~~~~~~~~~~~~~~~~~~WordCloud~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#  Bonus activity: can you filter and draw a Wordcloud of only the positive sentiment cases? Use also STOPWORDS from WordCloud

df_pos = data[data['airline_sentiment'] == "positive"]

wordcloud = WordCloud(stopwords = STOPWORDS,max_words = 1000 , width = 1600 , height = 800,
                     collocations=True).generate(" ".join(df_pos['final_text']))
plt.imshow(wordcloud)
------------------------------------------------------------------------------------------
#  Bonus activity: can you filter and draw a Wordcloud of only the negative sentiment cases? Use also STOPWORDS from WordCloud

df_neg = data[data['airline_sentiment'] == "negative"]

wordcloud = WordCloud(stopwords = STOPWORDS,max_words = 1000 , width = 1600 , height = 800,
                     collocations=True).generate(" ".join(df_neg['final_text']))
plt.imshow(wordcloud)
-----------------------------------------------------------------------------------------------
# Define a function named 'lowercase' to lowercase your text in data

def lowercase(text):
    return text.lower()
----------------------------------------------------------------------------------------------
# Apply your 'lowercase' function to lowercase your data on the 'text' feature of data.
# Remember to reassign back to the same column for the changes to take effect.
# Preview your results

data['text'] = data['text'].apply(lowercase)
data['text'].head()
-------------------------------------------------------------------------------------------------
# Define a function named 'remove_stopwords' to remove stopwords from your text.
# BONUS / Extra points: you may also wish to append to your stopwords the names of the airlines that you had previously stored in the variable 'airlines'
# You may also consider pre-processing these values before appending them.
# Optionally, you can also append any other frequently encountered words in your stopwords.

def remove_stopwords(text):
    textArr = text.split(' ')
    rem_text = " ".join([i for i in textArr if i not in stop_words])
    return rem_text
---------------------------------------------------------------------------------------
# Apply your 'remove_stopwords' function to remove the stopwords from your 'text' feature of data.
# Remember to reassign back to the same column for the changes to take effect.
# Preview your results

data['text'] = data['text'].apply(remove_stopwords)
data['text'].head()
------------------------------------------------------------------------------------------
# Define one or more functions to perform ****at least one more additional**** pre-processing step of your choice based on the data that you observe!
# The cleaner the data, the higher the mark for this activity :D

def remove_usernames(text):
    if isinstance(text, str):
        text = re.sub(r'@[^\s]+', '', text)
    return text
-------------------------------------------------------------------------------------------
# Justify and briefly describe your choice of pre-processing step(s) here

data['text'] = data['text'].apply(remove_usernames)
data['text'].head()

#i chose to remove the usernames in the comments because i didn't want to keep the mentioned names
----------------------------------------------------------------------------------------------
# Create a list of the 'text' column from data. Store in a new variable named 'text_list'.
# Preview the first entry of your text_list

text_list = data['text'].values.tolist()
text_list[0]
---------------------------------------------------------------------------------------
# Define a function to perform lemmatization, named 'lemmatization'.
# Bonus: allow postags only NOUN and ADJ

nlp = spacy.load('en_core_web_sm')
def lemmatization(text, allowed_postags=['NOUN', 'ADJ']): 
       output = []
       for sent in text:
            doc = nlp(sent) 
            output.append([token.lemma_ for token in doc if token.pos_ in allowed_postags ])
       return output
-------------------------------------------------------------------------------------------
# Run your lemmatization function on text_list (remember, this is a list, not a DataFrame so you cannot use apply())
# Store in a new variable named 'tokenized_reviews'
### THIS STEP MAY TAKE A WHILE TO EXECUTE ###

tokenized_reviews = lemmatization(text_list)
tokenized_reviews
------------------------------------------------------------------------------------------
# Concatenate the tokens into single sentences once again, creating a new feature within data named 'final_text'

data['final_text']= ''

for i in range(len(text_list)):
    data['final_text'].iloc[i] = "".join(text_list[i])
data.head()

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~Vectorization~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Store the final_text column from data as your variable X
# Store the airline_sentiment column from data as your class vector y

X = data['final_text']
y = data['airline_sentiment']
X
y
-------------------------------------------------------------------------------------------
# In this step we want to map the classes 'positive', 'neutral' and 'negative' to numerical values
# Instantiate a LabelEncoder() object and store in a variable named 'le'
# Fit and transform your LabelEncoder to your y class variable

le = LabelEncoder()
y = le.fit_transform(y)
y
-----------------------------------------------------------------------------------------
# Use the holdout method from sklearn to split your data into train and test. Set the random_state to 42 and use stratification.
# Print the dimensionality of the end results as a sanity check

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, stratify = y)
print('X_train shape: ', X_train.shape)
print('X_test shape: ', X_test.shape)
-----------------------------------------------------------------------------------------
# Import the library of the vectorization technique of your choice

from sklearn.feature_extraction.text import CountVectorizer
-----------------------------------------------------------------------
# Briefly justify your choice for this particular vectorizer

# i chose the “bags-of-words” representation which ignores structure and simply counts how often each word occurs. 
# CountVectorizer allows us to use the bags-of-words approach, by converting a collection of text documents into
# a matrix of token counts.
--------------------------------------------------------------------------
# Instantiate a vectorizer of your choice into a new variable named 'vect'

vect = CountVectorizer().fit(X_train)
vect
-------------------------------------------------------------------------
# Can you also create/ instantiate your vectorizer using unigrams and bigrams?
# Optional arguments to consider can be: min_df, max_df and max_features
# Store in a variable 'vect_grams'

vect_grams = CountVectorizer(min_df = 5, max_df = 20, ngram_range = (1,2)).fit(X_train)
-----------------------------------------------------------------------------------
# Apply any of the two afore-mentioned vectorizers to your train and test data to convert the text into numbers,
# and store the results in X_train_vec and X_test_vec respectively

X_train_vec = vect.transform(X_train)
X_test_vec = vect.transform(X_test)

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~Model Building~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Import the Supervised Learning algorithm (classification model) of your choice

from sklearn.svm import LinearSVC
-------------------------------------------------------------------------------------
# 1. Instantiate your ML model using any hyperparameters of your choice (no need for tuning)
# 2. Fit your model to your train data
# 3. Predict your test data. Store into a variable named 'pred'
# 4. Report the accuracy score between your predicted and real test values

SVCmodel = LinearSVC()
SVCmodel.fit(X_train_vec, y_train)
y_pred = SVCmodel.predict(X_test_vec)
-------------------------------------------------------------------------------------------
# Print the classification report

print(classification_report(y_test, y_pred))
-------------------------------------------------------------------------------------
# Build the confusion matrix between your predicted and real test values
# Store into a new variable named 'cm'

cm = confusion_matrix(y_test, y_pred)
cm
-----------------------------------------------------------------------------------
# Extra: plot in a heatmap the confusion matrix

f, ax = plt.subplots(figsize=(12, 8))
sns.heatmap(cm, 
            annot=True, 
            annot_kws={'size': 8}, 
            cmap="Spectral_r");

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~Unsupervised~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Create a corpora.Dictionary by passing the tokenized_reviews. Store in a new variable 'dictionary'

dictionary = corpora.Dictionary(tokenized_reviews)

texts = tokenized_reviews
print(tokenized_reviews[1])
-------------------------------------------------------------------------------------------------
# Create the doc_term_matrix from your tokenized_reviews

doc_term_matrix = [dictionary.doc2bow(rev) for rev in tokenized_reviews]
---------------------------------------------------------------------------------------------------
# Instantiate the LDA model and store it in a variable 'LDA''
# Build your LDA model using any parameters of your choice vy advising the documentation

# *********** Write comments next to every argument you have selected to use, justifying their entries/values

LDA = gensim.models.ldamodel.LdaModel

# Build LDA model
lda_model = LDA(corpus=doc_term_matrix,
                id2word=dictionary,
                num_topics=10, #set to 10
                random_state=100, #alpha and beta are hyperparameters that affect sparsity of the topics
                chunksize=100,
                passes=10, #controls how often we train the model on the entire corpus
                iterations=10) #controls how often we repeat a particular loop over each document
-----------------------------------------------------------------------------------------------------------------
# Print the Keywords for each of the number of topics generated by the lda_model

lda_model.print_topics()
--------------------------------------------------------------------------------------------------------------
# Create a visualization plot of your LDA

vis = gensimvis.prepare(lda_model, doc_term_matrix, dictionary)
vis
------------------------------------------------------------------------------------------------------------
# Calculate the coherence of your lda_model

from gensim.models import CoherenceModel

# Compute Coherence Score
coherence_model_lda = CoherenceModel(model=lda_model, texts=tokenized_reviews, dictionary=dictionary, coherence='c_v')
coherence_lda = coherence_model_lda.get_coherence()
print('Coherence Score: ', coherence_lda)
----------------------------------------------------------------------------------------------------------------
# Extra: calculate the perplexity of your lda_model

print('\nPerplexity: ', lda_model.log_perplexity(doc_term_matrix)) 
---------------------------------------------------------------------------------------------------------------


















